# ğŸ•µï¸â€â™‚ï¸ Fake Review Detector  
A multi-model system for detecting **computer-generated (CG)** vs **original (OR)** product reviews using NLP, topic modeling, and ensemble learning.

This project explores whether fake reviews can be detected by examining **category relevance**, **ratingâ€“text consistency**, and **linguistic style patterns**, and combines these signals into a weighted ensemble for robust classification.

---

## ğŸš€ Overview

Modern e-commerce platforms face an increasing surge of **AI-generated fake reviews**, which harm product credibility and user trust.  
This project builds a **3-model detection pipeline** that analyzes reviews from multiple perspectives:

### **1. Categoryâ€“Text Relevance Model (Model 1)**
- Hypothesis: Fake reviews are less semantically aligned with the product category.  
- Uses **LDA (topic modeling)** + **TF-IDF** category vectors.  
- Computes cosine similarity to score relevance.  
- Outputs a probability of a review being Original (OR) or Computer-Generated (CG).

### **2. Rating Consistency Model using BERT (Model 2)**
- Hypothesis: Fake reviews often have text that does not match their given rating.  
- Fine-tunes **BERT** to predict rating from text.  
- Computes the deviation between predicted and actual rating.  
- Larger deviation â‡’ more likely to be CG.

### **3. CBOW + Decision Tree Classifier (Model 3)**
- Hypothesis: CG reviews follow different stylistic patterns.  
- Uses **Bag-of-Words (CBOW embeddings)** + **Decision Tree classifier**.  
- Serves as a direct text-based baseline.

### **ğŸ”— Weighted Ensemble**
Instead of picking one model, this system combines all three using a **data-dependent weighting scheme**:

- Category relevance contributes more for certain categories.  
- Rating model has higher weight for low-rating reviews (1â€“3).  
- Remaining weight goes to the BoW-Decision Tree model.

This adaptive ensemble achieves better accuracy & generalization.

---

## ğŸ“Š Results

### **Validation Set**
| Model | Accuracy |
|------|----------|
| Model 3 only | 72.51% |
| Ensemble (M1 + M2 + M3) | **74.49%** |

### **Test Set**
| Model | Accuracy |
|------|----------|
| Model 3 only | 71.44% |
| Ensemble (M1 + M2 + M3) | **73.11%** |

The ensemble offers a clear improvement over individual models.

---

## ğŸ“‚ Project Structure

```
Fake_Review_Detector/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/       # Processed data files
â”‚   â”‚   â”œâ”€â”€ preprocessed_test.csv
â”‚   â”‚   â”œâ”€â”€ preprocessed_train.csv
â”‚   â”‚   â”œâ”€â”€ preprocessed_val.csv
â”‚   â”‚   â””â”€â”€ val_combined.csv
â”‚   â””â”€â”€ raw/             # Raw input data files
â”‚       â”œâ”€â”€ fake_reviews.csv
â”‚       â”œâ”€â”€ test.csv
â”‚       â”œâ”€â”€ train.csv
â”‚       â””â”€â”€ val.csv
â”‚
â”œâ”€â”€ notebooks/           # Jupyter notebooks for exploration and visualization
â”‚   â””â”€â”€ eda.ipynb
â”‚
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ config.py        # Configuration settings
â”‚   â”œâ”€â”€ embeddings/      # Text vectorization methods
â”‚   â”‚   â””â”€â”€ word2vec.py  # Bag of Words vectorization
â”‚   â”‚
â”‚   â”œâ”€â”€ ensemble/        # Ensemble learning implementation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_probs.csv
â”‚   â”‚   â”œâ”€â”€ val_probs.csv
â”‚   â”‚   â””â”€â”€ weighted_ensemble.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/          # ML model implementations
â”‚   â”‚   â”œâ”€â”€ decision_tree.py
â”‚   â”‚   â”œâ”€â”€ model1/
â”‚   â”‚   â”‚   â”œâ”€â”€ category_review_relevance.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ val.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dictionary.gensim
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ label_encoder.pkl
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lda_model.gensim
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lda_model.gensim.expElogbeta.npy
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lda_model.gensim.state
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ tfidf_vectorizer.pkl
â”‚   â”‚   â”‚   â”œâ”€â”€ threshold.py
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚   â”œâ”€â”€ val_category_relevance.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ val_results.csv
â”‚   â”‚   â”‚   â””â”€â”€ val_set_metrics.csv
â”‚   â”‚   â””â”€â”€ model2/
â”‚   â”‚       â”œâ”€â”€ dataset.csv
â”‚   â”‚       â”œâ”€â”€ metrics_with_threshold_per_rating.csv
â”‚   â”‚       â”œâ”€â”€ model_bert.py
â”‚   â”‚       â”œâ”€â”€ test.csv
â”‚   â”‚       â”œâ”€â”€ thresold.py
â”‚   â”‚       â”œâ”€â”€ train.csv
â”‚   â”‚       â””â”€â”€ val.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/   # Text preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ clean_text.py       # Text cleaning functions
â”‚   â”‚   â”œâ”€â”€ lemmatization.py    # Word lemmatization
â”‚   â”‚   â”œâ”€â”€ preprocessing_pipeline.py  # Complete pipeline
â”‚   â”‚   â””â”€â”€ stemming.py         # Word stemming
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py          # Main script to run the project
â”‚   â””â”€â”€ utils.py         # Utility functions
â”‚
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ Report.pdf           # Project report
```
